{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sequence_models_and_word_embeddings",
      "provenance": [],
      "mount_file_id": "16nqQY6dpgjQke_tCyOBkUC7RzaQcF95Z",
      "authorship_tag": "ABX9TyOXI8Ha5NGzShra4YFjBW1w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamim-hussain/sequence_models_and_word_embeddings/blob/main/sequence_models_and_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vla8VzOd2uzS"
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xvf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdpVfVIpv9hT",
        "outputId": "71e3d09b-07a5-4d34-bec3-83c358fa2fcf"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "base_path = Path('aclImdb')\n",
        "\n",
        "train_pos_txt = list(f.read_text(encoding=\"utf-8\") for f in (base_path/'train'/'pos').glob('*.txt'))\n",
        "train_neg_txt = list(f.read_text(encoding=\"utf-8\") for f in (base_path/'train'/'neg').glob('*.txt'))\n",
        "test_pos_txt = list(f.read_text(encoding=\"utf-8\") for f in (base_path/'test'/'pos').glob('*.txt'))\n",
        "test_neg_txt = list(f.read_text(encoding=\"utf-8\") for f in (base_path/'test'/'neg').glob('*.txt'))\n",
        "\n",
        "print('Positive sample 0:')\n",
        "print(train_pos_txt[0])\n",
        "print()\n",
        "print('Negative sample 0:')\n",
        "print(train_neg_txt[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sample 0:\n",
            "This movie is perfect for all the romantics in the world. John Ritter has never been better and has the best line in the movie! \"Sam\" hits close to home, is lovely to look at and so much fun to play along with. Ben Gazzara was an excellent cast and easy to fall in love with. I'm sure I've met Arthur in my travels somewhere. All around, an excellent choice to pick up any evening.!:-)\n",
            "\n",
            "Negative sample 0:\n",
            "There are movies like \"Plan 9\" that are so bad they have a charm about them, there are some like \"Waterworld\" that have the same inexplicable draw as a car accident, and there are some like \"Desperate living\" that you hate to admit you love. Cowgirls have none of these redemptions. The cast assembled has enough talent to make almost any plot watchable, and from what I've been told, the book is enjoyable.<br /><br />How then could this movie be so intolerably bad? To begin with, it seems the director brought together a cast of names with no other tie than what will bring in the 20 somethings. Then tell them to do their best Kevin Costner imitations. Open the book at random and start shooting whatever is on the page making sure to keep the wide expanses of America from being interesting in any way. Finally give the editing job to your brother-in-law, because the meat packing plant just laid him off. He does have twenty years of cutting experience.<br /><br />This movie now defines the basement for me. It is so bad, it isn't even good for being bad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8SnnBMNhVXs"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train_txt = train_pos_txt + train_neg_txt\n",
        "Y_train = np.array([1] * len(train_pos_txt) + [0] * len(train_neg_txt))\n",
        "\n",
        "X_test_txt = test_pos_txt + test_neg_txt\n",
        "Y_test = np.array([1] * len(test_pos_txt) + [0] * len(test_neg_txt))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad_5BgurfVRk",
        "outputId": "fb95c1eb-e49c-427c-b8ba-6ee3a3cbb36b"
      },
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "clean_regex = re.compile('<.*?>')  # To remove tags such as <br/></br> \n",
        "\n",
        "def text_data_cleaning(sentence):\n",
        "    sentence = re.sub(clean_regex, ' ', sentence)\n",
        "    doc = nlp(sentence)\n",
        "    \n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.lemma_ != \"-PRON-\":\n",
        "            temp = token.lemma_.lower().strip()\n",
        "        else:\n",
        "            temp = token.lower_\n",
        "        tokens.append(temp)\n",
        "    return tokens\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fnxhqd0tw4Gn",
        "outputId": "021a83b6-b8eb-43f5-c2a3-ed7f2c790982"
      },
      "source": [
        "print(text_data_cleaning(X_train_txt[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'movie', 'be', 'perfect', 'for', 'all', 'the', 'romantic', 'in', 'the', 'world', '.', 'john', 'ritter', 'have', 'never', 'be', 'well', 'and', 'have', 'the', 'good', 'line', 'in', 'the', 'movie', '!', '\"', 'sam', '\"', 'hit', 'close', 'to', 'home', ',', 'be', 'lovely', 'to', 'look', 'at', 'and', 'so', 'much', 'fun', 'to', 'play', 'along', 'with', '.', 'ben', 'gazzara', 'be', 'an', 'excellent', 'cast', 'and', 'easy', 'to', 'fall', 'in', 'love', 'with', '.', 'i', 'be', 'sure', 'i', 'have', 'meet', 'arthur', 'in', 'my', 'travel', 'somewhere', '.', 'all', 'around', ',', 'an', 'excellent', 'choice', 'to', 'pick', 'up', 'any', 'evening.!:-', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ciHqQ1Ljs1d"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    data = np.load('imdb.npz',allow_pickle=True)\n",
        "    X_train_tokens = data['X_train_tokens']\n",
        "    X_test_tokens = data['X_test_tokens']\n",
        "    data.close()\n",
        "except FileNotFoundError:\n",
        "    X_train_tokens = []\n",
        "    for sample in tqdm(X_train_txt, desc='Processing training files'):\n",
        "        X_train_tokens.append(text_data_cleaning(sample))\n",
        "\n",
        "    X_test_tokens = []\n",
        "    for sample in tqdm(X_test_txt, desc='Processing test files'):\n",
        "        X_test_tokens.append(text_data_cleaning(sample))\n",
        "\n",
        "    X_train_tokens=np.array(X_train_tokens, dtype=object)\n",
        "    X_test_tokens=np.array(X_test_tokens, dtype=object)\n",
        "\n",
        "    np.savez('imdb.npz', \n",
        "            X_train_tokens=X_train_tokens,\n",
        "            Y_train = Y_train,\n",
        "            X_test_tokens = X_test_tokens,\n",
        "            Y_test=Y_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBVh2Ct68_7y"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 2000\n",
        "max_len = 512\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<oov>')\n",
        "tokenizer.fit_on_texts(X_train_tokens)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train_tokens)\n",
        "X_test = tokenizer.texts_to_sequences(X_test_tokens)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, dtype='int32', padding='post',\n",
        "                        truncating='post', value=0)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len, dtype='int32', padding='post',\n",
        "                        truncating='post', value=0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaADwfvEAtJx",
        "outputId": "7cf70ed1-2995-471b-9040-4c90cf6bcac1"
      },
      "source": [
        "print(tokenizer.sequences_to_texts(X_train[0:1]))\n",
        "print(tokenizer.sequences_to_texts(X_train[-3:-2]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['red rock west ( <oov> )  <oov> cage get <oov> in a <oov> crime without at first know it , and the <oov> lead to <oov> <oov> , adventure and <oov> in the wild <oov> american west of the <oov> . red rock west be often brutal and sometimes hilarious , and cage pull off the <oov> with his usual <oov> wit and <oov> <oov> .  be the plot over the top ? yes . be <oov> <oov> perfect as a <oov> , almost likable killer ? yes . do cage stand a chance ? well , you have to watch and see . it never let up , and it take me by surprise the first time i see it . on second viewing <oov> , i be surprised at how well it hold up , how well <oov> it be , and how <oov> and funny it be at the same time .  director <oov> <oov> ( who also help write ) be know more for his tv work , but with <oov> and this film he show a <oov> hand with <oov> plot . it be save by its humor by the way , and by the <oov> . the bar be <oov> , the cop <oov> . and do not miss a really inspire cameo by <oov> <oov> as a <oov> <oov> . <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov>']\n",
            "['the only reason \" the <oov> <oov> \" deserve any star be the presence of <oov> <oov> in the cast . other than get to see <oov> baby in a <oov> woman \" performance , there be nothing else worth see here .  the <oov> :  * * * minor spoiler * * *  david <oov> be <oov> to write a book on the <oov> . one day he <oov> disappear after phone his <oov> and suggest he have <oov> across something that have place him in <oov> danger . the entire series for which this <oov> pilot be write be apparently intend to be a series of flashback <oov> the \" <oov> tape \" -- a set of <oov> tape the writer record while <oov> case of the <oov> .  in the pilot episode , a <oov> <oov> die -- but not before <oov> an ancient <oov> <oov> ring from a local <oov> who <oov> him the <oov> will give him <oov> after death . we soon discover the ring itself do not grant <oov> . instead , it only <oov> the <oov> \\'s <oov> , allow him to escape his <oov> so he can run around town <oov> pretty girl of all their blood .  blue - face , <oov> - <oov> and <oov> out <oov> you have not hear since you last watch \" <oov> <oov> \" cartoon , the <oov> attack his wife ( <oov> ) one dark night . she escape and , <oov> a <oov> <oov> , <oov> <oov> to get his <oov> in <oov> the mystery behind her late husband \\'s <oov> <oov> on her <oov> .  so be this guy a vampire ? no . there be no vampire in this story despite what you may have read or hear . the <oov> do not drink his victim \\' blood -- he <oov> it . how ? do not know . we only see him attack , never <oov> . why ? to <oov> the second part of his <oov> for <oov> . it turn out the <oov> <oov> will only be allow to live forever if he build a life - <oov> <oov> of a demon name <oov> out of a <oov> of <oov> and human blood . once it be finish , <oov> plan to <oov> the <oov> , use it as his <oov> into our world .  the final <oov> :  \" the <oov> <oov> \" be not pick up as a series for a very good reason ... it be garbage . as you can see from my <oov> , the story be a <oov> , ill - <oov> mess . the acting and dialogue offer no well . this be not even a <oov> for a \" so bad it be good \" award . sometimes bad be just bad .  after year of hear <oov> <oov> about the great <oov> of \" the <oov> <oov> , \" i be very <oov> to find it air on fox movie channel']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw5DRwktli-e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,  X_val, Y_train, Y_val = train_test_split(X_train, Y_train,\n",
        "                                                  test_size=.075, stratify=Y_train,\n",
        "                                                  shuffle=True,random_state=123)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmybYqmwSJrM",
        "outputId": "069f5417-22ab-48ab-f422-220caa060cac"
      },
      "source": [
        "import os\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_file_path = f'glove.6B.{embedding_dim}d.txt'\n",
        "\n",
        "if not os.path.exists(embedding_file_path):\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip -q glove.6B.zip\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(embedding_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "    for line in tqdm(f):\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"\\nFound %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "embedding_matrix = np.random.uniform(size=(vocab_size, embedding_dim)).astype('float32')\n",
        "\n",
        "for i in range(vocab_size):\n",
        "    try:\n",
        "        embedding_matrix[i] = embeddings_index[tokenizer.index_word[i]]\n",
        "    except KeyError:\n",
        "        pass"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400000it [00:07, 50963.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 400000 word vectors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNcpbLoS4uKa",
        "outputId": "6a1cc0ec-c1bd-4845-80c0-69616608f72a"
      },
      "source": [
        "from tensorflow.keras import models, layers, initializers, optimizers, losses, metrics\n",
        "\n",
        "model_layers = []\n",
        "model_layers.append(layers.Embedding(vocab_size, \n",
        "                               embedding_dim, \n",
        "                               embeddings_initializer=initializers.Constant(embedding_matrix), \n",
        "                               mask_zero=True,\n",
        "                               input_shape=[max_len]))\n",
        "model_layers.append(layers.LSTM(128))\n",
        "model_layers.append(layers.Dense(1, activation=None))\n",
        "\n",
        "model = models.Sequential(model_layers, name='lstm_model')\n",
        "\n",
        "loss = losses.BinaryCrossentropy(from_logits=True)\n",
        "acc = metrics.BinaryAccuracy(name='acc')\n",
        "optim = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(optimizer=optim, loss=loss, metrics=[acc])\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"lstm_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 512, 100)          200000    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 317,377\n",
            "Trainable params: 317,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaRVgxeI8AXJ",
        "outputId": "5a2453c9-529b-4c78-b679-09dca63a5a2a"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, epochs=5, validation_data=(X_val,Y_val))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "723/723 [==============================] - 84s 110ms/step - loss: 0.5558 - acc: 0.6845 - val_loss: 0.3908 - val_acc: 0.8107\n",
            "Epoch 2/5\n",
            "723/723 [==============================] - 75s 104ms/step - loss: 0.3293 - acc: 0.8570 - val_loss: 0.3216 - val_acc: 0.8709\n",
            "Epoch 3/5\n",
            "723/723 [==============================] - 74s 103ms/step - loss: 0.2737 - acc: 0.8858 - val_loss: 0.2995 - val_acc: 0.8773\n",
            "Epoch 4/5\n",
            "723/723 [==============================] - 75s 103ms/step - loss: 0.2403 - acc: 0.9013 - val_loss: 0.3082 - val_acc: 0.8880\n",
            "Epoch 5/5\n",
            "723/723 [==============================] - 75s 103ms/step - loss: 0.2122 - acc: 0.9149 - val_loss: 0.2900 - val_acc: 0.8885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc000cf0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbIrvC3z_O1y",
        "outputId": "b4ec2826-d931-40fa-976c-57e0d0cb7c37"
      },
      "source": [
        "scores= model.evaluate(X_test, Y_test)\n",
        "scores"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 30s 38ms/step - loss: 0.2746 - acc: 0.8914\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.27464592456817627, 0.8914399743080139]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRNkdZZFI6le",
        "outputId": "48e7046b-ee1f-4de1-fe17-1c15ed05c8b4"
      },
      "source": [
        "from tensorflow.keras import models, layers, initializers, optimizers, losses, metrics\n",
        "\n",
        "model_layers = []\n",
        "model_layers.append(layers.Embedding(vocab_size, \n",
        "                               embedding_dim, \n",
        "                               embeddings_initializer=initializers.Constant(embedding_matrix), \n",
        "                               mask_zero=True,\n",
        "                               input_shape=[max_len]))\n",
        "model_layers.append(layers.SimpleRNN(256))\n",
        "model_layers.append(layers.Dense(1, activation=None))\n",
        "\n",
        "model = models.Sequential(model_layers, name='lstm_model')\n",
        "\n",
        "loss = losses.BinaryCrossentropy(from_logits=True)\n",
        "acc = metrics.BinaryAccuracy(name='acc')\n",
        "optim = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(optimizer=optim, loss=loss, metrics=[acc])\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"lstm_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 512, 100)          200000    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 256)               91392     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 291,649\n",
            "Trainable params: 291,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbtRvD2nJHHt",
        "outputId": "969e12e7-f481-4252-ee53-89c40b032fc2"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, epochs=5, validation_data=(X_val,Y_val))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "723/723 [==============================] - 782s 1s/step - loss: 0.6922 - acc: 0.5293 - val_loss: 0.6517 - val_acc: 0.5717\n",
            "Epoch 2/5\n",
            "723/723 [==============================] - 783s 1s/step - loss: 0.6523 - acc: 0.5849 - val_loss: 0.6106 - val_acc: 0.6715\n",
            "Epoch 3/5\n",
            "723/723 [==============================] - 772s 1s/step - loss: 0.6231 - acc: 0.6249 - val_loss: 0.5940 - val_acc: 0.6645\n",
            "Epoch 4/5\n",
            "723/723 [==============================] - 767s 1s/step - loss: 0.6090 - acc: 0.6391 - val_loss: 0.6607 - val_acc: 0.5659\n",
            "Epoch 5/5\n",
            "723/723 [==============================] - 769s 1s/step - loss: 0.6082 - acc: 0.6380 - val_loss: 0.6026 - val_acc: 0.7216\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc01a21750>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbNXsvfqZ_7s",
        "outputId": "f3ff4047-80f0-4424-b938-24b8bc24e288"
      },
      "source": [
        "loss, acc = model.evaluate(X_test, Y_test)\n",
        "print(f'Test accuracy = {acc:0.3%}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 59s 76ms/step - loss: 0.6062 - acc: 0.7160\n",
            "Test accuracy = 71.604%\n"
          ]
        }
      ]
    }
  ]
}
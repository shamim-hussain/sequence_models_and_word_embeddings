{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sequence_models_and_word_embeddings",
      "provenance": [],
      "collapsed_sections": [
        "6hUDcEwVmm2J",
        "2yZdQUigmh3k"
      ],
      "toc_visible": true,
      "mount_file_id": "16nqQY6dpgjQke_tCyOBkUC7RzaQcF95Z",
      "authorship_tag": "ABX9TyPaS9nUl6OQmZsALdVL3kBd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shamim-hussain/sequence_models_and_word_embeddings/blob/main/sequence_models_and_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enQ1wb5HcMn8"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftl96J1vcR7S"
      },
      "source": [
        "## Targeted Task\n",
        "\n",
        "We will perform sentiment analysis on the **IMDB movie reviews dataset**. It is a dataset of 50000 (text) reviews which are categorized as either positive or negative depending on the review scores. The link to the dataset:\n",
        "\n",
        "https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "This problem is appropriate for sequence modeling because the sequence in which the words appear in the text is important to properly represent their meaning. So we will apply a Recurrent Neural Network on the word embeddings in the text in a sequential manner and the classification will be performed based on the final output of the RNN, which should represent the overall sentiment expressed within the text.\n",
        "\n",
        "We will use **Tensorflow** with the **Keras** high level framework to implement our model. We will also use the pretrained GloVe word embeddings to initialize our input word embeddings in order to get better classification results via transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vqP_7KEd_7P"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHvgnuE3eFF3"
      },
      "source": [
        "### Download and read text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Rqtip9reTV3"
      },
      "source": [
        "First we will download and extract the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vla8VzOd2uzS"
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xvf aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg7Jy3GMeWPv"
      },
      "source": [
        "Next we will read the text review files, both positive and negative."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdpVfVIpv9hT",
        "outputId": "71e3d09b-07a5-4d34-bec3-83c358fa2fcf"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "base_path = Path('aclImdb')\n",
        "\n",
        "train_pos_txt = list(f.read_text(encoding=\"utf-8\") for f in (base_path/'train'/'pos').glob('*.txt'))\n",
        "train_neg_txt = list(f.read_text(encoding=\"utf-8\") for f in (base_path/'train'/'neg').glob('*.txt'))\n",
        "test_pos_txt = list(f.read_text(encoding=\"utf-8\") for f in (base_path/'test'/'pos').glob('*.txt'))\n",
        "test_neg_txt = list(f.read_text(encoding=\"utf-8\") for f in (base_path/'test'/'neg').glob('*.txt'))\n",
        "\n",
        "print('Positive sample 0:')\n",
        "print(train_pos_txt[0])\n",
        "print()\n",
        "print('Negative sample 0:')\n",
        "print(train_neg_txt[0])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sample 0:\n",
            "This movie is perfect for all the romantics in the world. John Ritter has never been better and has the best line in the movie! \"Sam\" hits close to home, is lovely to look at and so much fun to play along with. Ben Gazzara was an excellent cast and easy to fall in love with. I'm sure I've met Arthur in my travels somewhere. All around, an excellent choice to pick up any evening.!:-)\n",
            "\n",
            "Negative sample 0:\n",
            "There are movies like \"Plan 9\" that are so bad they have a charm about them, there are some like \"Waterworld\" that have the same inexplicable draw as a car accident, and there are some like \"Desperate living\" that you hate to admit you love. Cowgirls have none of these redemptions. The cast assembled has enough talent to make almost any plot watchable, and from what I've been told, the book is enjoyable.<br /><br />How then could this movie be so intolerably bad? To begin with, it seems the director brought together a cast of names with no other tie than what will bring in the 20 somethings. Then tell them to do their best Kevin Costner imitations. Open the book at random and start shooting whatever is on the page making sure to keep the wide expanses of America from being interesting in any way. Finally give the editing job to your brother-in-law, because the meat packing plant just laid him off. He does have twenty years of cutting experience.<br /><br />This movie now defines the basement for me. It is so bad, it isn't even good for being bad.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNxCJfPPej_f"
      },
      "source": [
        "Let us combine the positive and negative reviews and add corresponding labels to them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8SnnBMNhVXs"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X_train_txt = train_pos_txt + train_neg_txt\n",
        "Y_train = np.array([1] * len(train_pos_txt) + [0] * len(train_neg_txt))\n",
        "\n",
        "X_test_txt = test_pos_txt + test_neg_txt\n",
        "Y_test = np.array([1] * len(test_pos_txt) + [0] * len(test_neg_txt))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SHFBLg-eyun"
      },
      "source": [
        "### Lemmatization and Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yImlvaCufJnX"
      },
      "source": [
        "We cannot train our model with the raw text. First we will separate the tokens and optionally lemmatize them. Lemmatization will reduce each word to their base form and thus allow for a smaller vocabulary. This will save resources and also possibly prevent overfitting.\n",
        "\n",
        "We will use the SpaCy library for lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad_5BgurfVRk",
        "outputId": "fb95c1eb-e49c-427c-b8ba-6ee3a3cbb36b"
      },
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "clean_regex = re.compile('<.*?>')  # To remove tags such as <br/></br> \n",
        "\n",
        "def text_data_cleaning(sentence):\n",
        "    sentence = re.sub(clean_regex, ' ', sentence)\n",
        "    doc = nlp(sentence)\n",
        "    \n",
        "    tokens = []\n",
        "    for token in doc:\n",
        "        if token.lemma_ != \"-PRON-\":\n",
        "            temp = token.lemma_.lower().strip()\n",
        "        else:\n",
        "            temp = token.lower_\n",
        "        tokens.append(temp)\n",
        "    return tokens\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.62.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kml70Ud5f3JF"
      },
      "source": [
        "Let us look at the a sample lemmatized text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fnxhqd0tw4Gn",
        "outputId": "71a453f5-1cc7-4d3d-9a7b-adde7d07aa23"
      },
      "source": [
        "print('Original:')\n",
        "print(X_train_txt[0])\n",
        "print('Tokenized and lemmatized:')\n",
        "print(text_data_cleaning(X_train_txt[0]))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:\n",
            "This movie is perfect for all the romantics in the world. John Ritter has never been better and has the best line in the movie! \"Sam\" hits close to home, is lovely to look at and so much fun to play along with. Ben Gazzara was an excellent cast and easy to fall in love with. I'm sure I've met Arthur in my travels somewhere. All around, an excellent choice to pick up any evening.!:-)\n",
            "Tokenized and lemmatized:\n",
            "['this', 'movie', 'be', 'perfect', 'for', 'all', 'the', 'romantic', 'in', 'the', 'world', '.', 'john', 'ritter', 'have', 'never', 'be', 'well', 'and', 'have', 'the', 'good', 'line', 'in', 'the', 'movie', '!', '\"', 'sam', '\"', 'hit', 'close', 'to', 'home', ',', 'be', 'lovely', 'to', 'look', 'at', 'and', 'so', 'much', 'fun', 'to', 'play', 'along', 'with', '.', 'ben', 'gazzara', 'be', 'an', 'excellent', 'cast', 'and', 'easy', 'to', 'fall', 'in', 'love', 'with', '.', 'i', 'be', 'sure', 'i', 'have', 'meet', 'arthur', 'in', 'my', 'travel', 'somewhere', '.', 'all', 'around', ',', 'an', 'excellent', 'choice', 'to', 'pick', 'up', 'any', 'evening.!:-', ')']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhCry4SzgXs1"
      },
      "source": [
        "We see that the basic sense of the text is still conveyed by the lemmatized tokens.\n",
        "\n",
        "Now we will tokenize and lemmatize the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ciHqQ1Ljs1d"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    data = np.load('imdb.npz',allow_pickle=True)\n",
        "    X_train_tokens = data['X_train_tokens']\n",
        "    X_test_tokens = data['X_test_tokens']\n",
        "    data.close()\n",
        "except FileNotFoundError:\n",
        "    X_train_tokens = []\n",
        "    for sample in tqdm(X_train_txt, desc='Processing training files'):\n",
        "        X_train_tokens.append(text_data_cleaning(sample))\n",
        "\n",
        "    X_test_tokens = []\n",
        "    for sample in tqdm(X_test_txt, desc='Processing test files'):\n",
        "        X_test_tokens.append(text_data_cleaning(sample))\n",
        "\n",
        "    X_train_tokens=np.array(X_train_tokens, dtype=object)\n",
        "    X_test_tokens=np.array(X_test_tokens, dtype=object)\n",
        "\n",
        "    np.savez('imdb.npz', \n",
        "            X_train_tokens=X_train_tokens,\n",
        "            Y_train = Y_train,\n",
        "            X_test_tokens = X_test_tokens,\n",
        "            Y_test=Y_test)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oLOFSKg9hN"
      },
      "source": [
        "## Building vocabulary and conversion into sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF-5DXvtih7j"
      },
      "source": [
        "We will limit our vocabulary to 2000 most frequent tokens. Also we will truncate and pad the sequences so that they have a uniform length of 512 tokens. The padded parts (with a value of 0) will be masked and the model will effectively ignore these parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBVh2Ct68_7y"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "vocab_size = 2000\n",
        "max_len = 512\n",
        "\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<oov>')\n",
        "tokenizer.fit_on_texts(X_train_tokens)\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(X_train_tokens)\n",
        "X_test = tokenizer.texts_to_sequences(X_test_tokens)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, dtype='int32', padding='post',\n",
        "                        truncating='post', value=0)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len, dtype='int32', padding='post',\n",
        "                        truncating='post', value=0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1fv8LNZjIVL"
      },
      "source": [
        "The text can be partiallly reconstructed from the sequences by looking up the vocabulary. For example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaADwfvEAtJx",
        "outputId": "cdb59724-581a-4016-9ffe-34b206e88dab"
      },
      "source": [
        "print('Sequence:')\n",
        "print(X_train[0])\n",
        "print('Reconstructed:')\n",
        "print(tokenizer.sequences_to_texts(X_train[0:1])[0])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence:\n",
            "[   2    1   93   37  153  343    4   19   13   21    3  556  529   41\n",
            " 1744    1    5   26  120   70   10   12   58   15  546   55  147    4\n",
            "    2   21    3  421  212    6  283    4   10   76   17  230  405   38\n",
            "   59    5   62  602  256    5    6    7  874   16 1215   16  142    4\n",
            "   26   10   52    3   15  528    4   10   98   25   15   39    7  235\n",
            "  157    8  280    5   11  124   26  276   11   67    1    4   12   25\n",
            "   15   96  267   14    1   42  144   67   25    4   19   13    3    7\n",
            "  851   20    9  107   28  181   12  896   29   25   15 1010    2 1446\n",
            "    4    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0]\n",
            "Reconstructed:\n",
            "the <oov> : see something else .  this film be highly rate by gene <oov> , but after watch it i can not figure out why . the film be definitely original and different . it even have interesting dialogue at time , some cool moment , and a creepy \" noir \" feel . but it just be not entertaining . it also do not make a whole lot of sense , in plot but especially in character <oov> . i do not know anyone that <oov> like these character do .  this be a difficult movie to take on -- i suggest you do not accept the challenge . <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov> <oov>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si3yQYa7j_aa"
      },
      "source": [
        "### Train-Validation split\n",
        "Let us split the training dataset to reserve some data for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw5DRwktli-e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,  X_val, Y_train, Y_val = train_test_split(X_train, Y_train,\n",
        "                                                  test_size=.075, stratify=Y_train,\n",
        "                                                  shuffle=True,random_state=123)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzIVh_RXkd_B"
      },
      "source": [
        "## Model Definition, Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvgH6nkiknUr"
      },
      "source": [
        "### Preparing pretrained word embeddings\n",
        "We will use the GloVe word embeddings to initialize the input word embeddings. These embeddings are openly available at:\n",
        "\n",
        "https://nlp.stanford.edu/projects/glove/\n",
        "\n",
        "We will (downlad and) match each word in our vocabulary to the embedding vector provided for that word to form an embedding matrix. If that particular word/token is not present, it is randomly initialized."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmybYqmwSJrM",
        "outputId": "069f5417-22ab-48ab-f422-220caa060cac"
      },
      "source": [
        "import os\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "embedding_file_path = f'glove.6B.{embedding_dim}d.txt'\n",
        "\n",
        "if not os.path.exists(embedding_file_path):\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip -q glove.6B.zip\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(embedding_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "    for line in tqdm(f):\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"\\nFound %s word vectors.\" % len(embeddings_index))\n",
        "\n",
        "embedding_matrix = np.random.uniform(size=(vocab_size, embedding_dim)).astype('float32')\n",
        "\n",
        "for i in range(vocab_size):\n",
        "    try:\n",
        "        embedding_matrix[i] = embeddings_index[tokenizer.index_word[i]]\n",
        "    except KeyError:\n",
        "        pass"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400000it [00:07, 50963.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Found 400000 word vectors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hUDcEwVmm2J"
      },
      "source": [
        "### Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRNkdZZFI6le",
        "outputId": "48e7046b-ee1f-4de1-fe17-1c15ed05c8b4"
      },
      "source": [
        "from tensorflow.keras import models, layers, initializers, optimizers, losses, metrics\n",
        "\n",
        "model_layers = []\n",
        "model_layers.append(layers.Embedding(vocab_size, \n",
        "                               embedding_dim, \n",
        "                               embeddings_initializer=initializers.Constant(embedding_matrix), \n",
        "                               mask_zero=True,\n",
        "                               input_shape=[max_len]))\n",
        "model_layers.append(layers.SimpleRNN(256))\n",
        "model_layers.append(layers.Dense(1, activation=None))\n",
        "\n",
        "model = models.Sequential(model_layers, name='lstm_model')\n",
        "\n",
        "loss = losses.BinaryCrossentropy(from_logits=True)\n",
        "acc = metrics.BinaryAccuracy(name='acc')\n",
        "optim = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(optimizer=optim, loss=loss, metrics=[acc])\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"lstm_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 512, 100)          200000    \n",
            "_________________________________________________________________\n",
            "simple_rnn (SimpleRNN)       (None, 256)               91392     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 291,649\n",
            "Trainable params: 291,649\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbtRvD2nJHHt",
        "outputId": "969e12e7-f481-4252-ee53-89c40b032fc2"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, epochs=5, validation_data=(X_val,Y_val))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "723/723 [==============================] - 782s 1s/step - loss: 0.6922 - acc: 0.5293 - val_loss: 0.6517 - val_acc: 0.5717\n",
            "Epoch 2/5\n",
            "723/723 [==============================] - 783s 1s/step - loss: 0.6523 - acc: 0.5849 - val_loss: 0.6106 - val_acc: 0.6715\n",
            "Epoch 3/5\n",
            "723/723 [==============================] - 772s 1s/step - loss: 0.6231 - acc: 0.6249 - val_loss: 0.5940 - val_acc: 0.6645\n",
            "Epoch 4/5\n",
            "723/723 [==============================] - 767s 1s/step - loss: 0.6090 - acc: 0.6391 - val_loss: 0.6607 - val_acc: 0.5659\n",
            "Epoch 5/5\n",
            "723/723 [==============================] - 769s 1s/step - loss: 0.6082 - acc: 0.6380 - val_loss: 0.6026 - val_acc: 0.7216\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc01a21750>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbNXsvfqZ_7s",
        "outputId": "f3ff4047-80f0-4424-b938-24b8bc24e288"
      },
      "source": [
        "loss, acc = model.evaluate(X_test, Y_test)\n",
        "print(f'Test accuracy = {acc:0.3%}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 59s 76ms/step - loss: 0.6062 - acc: 0.7160\n",
            "Test accuracy = 71.604%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yZdQUigmh3k"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNcpbLoS4uKa",
        "outputId": "6a1cc0ec-c1bd-4845-80c0-69616608f72a"
      },
      "source": [
        "from tensorflow.keras import models, layers, initializers, optimizers, losses, metrics\n",
        "\n",
        "model_layers = []\n",
        "model_layers.append(layers.Embedding(vocab_size, \n",
        "                               embedding_dim, \n",
        "                               embeddings_initializer=initializers.Constant(embedding_matrix), \n",
        "                               mask_zero=True,\n",
        "                               input_shape=[max_len]))\n",
        "model_layers.append(layers.LSTM(128))\n",
        "model_layers.append(layers.Dense(1, activation=None))\n",
        "\n",
        "model = models.Sequential(model_layers, name='lstm_model')\n",
        "\n",
        "loss = losses.BinaryCrossentropy(from_logits=True)\n",
        "acc = metrics.BinaryAccuracy(name='acc')\n",
        "optim = optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "model.compile(optimizer=optim, loss=loss, metrics=[acc])\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"lstm_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 512, 100)          200000    \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               117248    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 317,377\n",
            "Trainable params: 317,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaRVgxeI8AXJ",
        "outputId": "5a2453c9-529b-4c78-b679-09dca63a5a2a"
      },
      "source": [
        "model.fit(X_train, Y_train, batch_size=32, epochs=5, validation_data=(X_val,Y_val))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "723/723 [==============================] - 84s 110ms/step - loss: 0.5558 - acc: 0.6845 - val_loss: 0.3908 - val_acc: 0.8107\n",
            "Epoch 2/5\n",
            "723/723 [==============================] - 75s 104ms/step - loss: 0.3293 - acc: 0.8570 - val_loss: 0.3216 - val_acc: 0.8709\n",
            "Epoch 3/5\n",
            "723/723 [==============================] - 74s 103ms/step - loss: 0.2737 - acc: 0.8858 - val_loss: 0.2995 - val_acc: 0.8773\n",
            "Epoch 4/5\n",
            "723/723 [==============================] - 75s 103ms/step - loss: 0.2403 - acc: 0.9013 - val_loss: 0.3082 - val_acc: 0.8880\n",
            "Epoch 5/5\n",
            "723/723 [==============================] - 75s 103ms/step - loss: 0.2122 - acc: 0.9149 - val_loss: 0.2900 - val_acc: 0.8885\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efc000cf0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbIrvC3z_O1y",
        "outputId": "b4ec2826-d931-40fa-976c-57e0d0cb7c37"
      },
      "source": [
        "scores= model.evaluate(X_test, Y_test)\n",
        "scores"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 30s 38ms/step - loss: 0.2746 - acc: 0.8914\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.27464592456817627, 0.8914399743080139]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ]
}